{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4a1d567",
   "metadata": {},
   "source": [
    "# Transformer Attention\n",
    "\n",
    "- Attention Mechanism : 특정 Layer층에서 출력되는 매 시점마다 전체 문장을 다시 참조하여 출력하는 학습 형태 \n",
    "- 유사도를 계산하여, 가장 유사도가 높은 문장의 벡터를 찾고, 다음 Layer 반영 \n",
    "- 어텐션 매커니즘의 핵심 개념 : \n",
    "    - 키 (Key) : 비교 대상이 되는 단어들의 표현 \n",
    "    - 값 (Value) : 현재 처리중인 문장에 대한 대상 표현 \n",
    "    - 쿼리 (Query) : 질문과 응답간에 얻어진 유사도 값 \n",
    "    \n",
    "- 트랜스포머에서 사용되는 핵심 Attention \n",
    "    - Muti-Head Attetion : 인코더와 디코더의 어탠션을 \"헤드\"요소를 통해 병렬 \n",
    "        - Encoder Self Attention (Masked Decoder Self Attention) : 모델이 하나의 시퀀스 내에서 단어들 사이의 관계를 파악 \n",
    "        - Encoder-Decoder Attention : 모델이 인코더의 출력과 디코더의 입력 사이의 관계를 파악 \n",
    "    \n",
    "    ![image2](https://wikidocs.net/images/page/31379/transformer_attention_overview.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "665059bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fab3615b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rlack\\AppData\\Local\\Temp\\ipykernel_1968\\1765498031.py:2: load (from tensorflow.python.data.experimental.ops.io) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.load(...)` instead.\n"
     ]
    }
   ],
   "source": [
    "# 질의응답 데이터를 포지셔널 인코딩을 통해 위치정보가 포함된 텐서 데이터 셋 \n",
    "save_path = '/chatbot_dataset'\n",
    "dataset3 = tf.data.experimental.load(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "879ed246",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_LoadDataset element_spec=({'inputs': TensorSpec(shape=(None, 40), dtype=tf.int32, name=None), 'dec_inputs': TensorSpec(shape=(None, 39), dtype=tf.int32, name=None)}, {'outputs': TensorSpec(shape=(None, 39), dtype=tf.int32, name=None)})>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a403c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN = [8178] # <SOS> \n",
    "END_TOKEN = [8179]   # <EOS>\n",
    "VOCAB_SIZE = 8180  # 단어가 하나의 정수로 매칭된 사전의 크기 \n",
    "MAX_LENGTH = 40 # 하나의 문장이 정수의 토큰들로 표현되는 열의 크기  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33f0cc6",
   "metadata": {},
   "source": [
    "**Chat Bot 구현**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13d945e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3de3da6",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rlack\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend\\common\\global_state.py:73: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# 하이퍼파라미터를 지정 \n",
    "D_MODEL = 256 # 트랜스 포머 내 모든 Layer안에 Node 수 \n",
    "NUM_LAYERS = 2 # 인코더와 디코더 각각에 있는 Layer 수 \n",
    "NUM_HEADS = 8 # 멀티-헤드 어텐션에서 사용되는 헤드의 수 \n",
    "DFF = 512 # Feed Forward 신경망 내 Layer Node 수 \n",
    "DROPOUT = 0.1 # 드롭아웃의 비율 \n",
    "\n",
    "model = transformer_chatbot.transformer(vocab_size=VOCAB_SIZE, \n",
    "                                       num_layers=NUM_LAYERS,\n",
    "                                       dff=DFF,\n",
    "                                       d_model=D_MODEL,\n",
    "                                       dropout=DROPOUT,\n",
    "                                       num_heads=NUM_HEADS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3e0506e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 컴파일을 위한 정확도 계산 함수를 생성 \n",
    "# 문장의 최대 길이에서 시작 토큰과 종료 토큰을 제외하고 나머지 데이터를 이용해 \n",
    "# 정확도를 계산 -> Weight Update \n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH-1))\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b6f1234",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  최적화 객체 생성  / 각 Node 수에 맞게 끔 Learning Rate 세팅 \n",
    "learning_rate = transformer_chatbot.CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, \n",
    "                                     beta_1=0.9,\n",
    "                                     beta_2=0.98, epsilon=1e-9)\n",
    "# adam : RMSprop 과 모멘텀 기법을 결합하여, 학습율도 조절하면서 모멘텀 벡터도 계산\n",
    "# 각 최적화 함수의 이동평균(Moving Average)값을 이용해 Weight 최적화 \n",
    "# beta : 모멘텀 및 스케일 조정 계수 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3870c9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, \n",
    "              loss=transformer_chatbot.loss_function,\n",
    "               metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "604a303b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 2s/step - accuracy: 0.0199 - loss: 1.4217\n",
      "Epoch 2/10\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m531s\u001b[0m 3s/step - accuracy: 0.0485 - loss: 1.1541\n",
      "Epoch 3/10\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m478s\u001b[0m 3s/step - accuracy: 0.0493 - loss: 0.9600\n",
      "Epoch 4/10\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m468s\u001b[0m 2s/step - accuracy: 0.0516 - loss: 0.8803\n",
      "Epoch 5/10\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m528s\u001b[0m 3s/step - accuracy: 0.0546 - loss: 0.8264\n",
      "Epoch 6/10\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m487s\u001b[0m 2s/step - accuracy: 0.0574 - loss: 0.7771\n",
      "Epoch 7/10\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 755ms/step - accuracy: 0.0611 - loss: 0.7228\n",
      "Epoch 8/10\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 907ms/step - accuracy: 0.0659 - loss: 0.6666\n",
      "Epoch 9/10\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 920ms/step - accuracy: 0.0727 - loss: 0.6046\n",
      "Epoch 10/10\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 765ms/step - accuracy: 0.0800 - loss: 0.5412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x20ac57e0ed0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset3, epochs=10) # 약 100회정도 학습 시, 대화 통하는 정도의 모델 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9b00907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력값 처리 \n",
    "def preprocess_sentence(sentence):\n",
    "    # 입력 문장에 특수기호 처리 \n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    return sentence.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f664c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    # 입력 문장에 대한 전처리\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    # 입력 문장에 시작 토큰과 종료 토큰을 추가\n",
    "    sentence = tf.expand_dims(\n",
    "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "    output = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "    # 디코더의 예측 시작\n",
    "    for i in range(MAX_LENGTH):\n",
    "        predictions = model(inputs=[sentence, output], training=False)\n",
    "\n",
    "        # 모델의 출력에서 마지막 단어를 선택해, 이를 바탕으로 다음 단어를 예측 \n",
    "        # 문장내 단어를 하나씩 출력         \n",
    "        predictions = predictions[:, -1:, :]\n",
    "        # 예측된 단어를 변수로 선언 \n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        # 만약 현재 시점의 예측 단어가 종료 토큰이라면 예측을 중단\n",
    "        if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "              break\n",
    "\n",
    "        # 현재 시점의 예측 단어를 output(출력)에 연결한다.\n",
    "        # output은 for문의 다음 루프에서 디코더의 입력이 된다.\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    # 단어 예측이 모두 끝났다면 output을 리턴.\n",
    "    return tf.squeeze(output, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d23b1602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence):\n",
    "    prediction = evaluate(sentence)\n",
    "    # 앞서 출력된 Matrix의 결과값을 하나씩 가져와, 단어사전에 있는 문자로 변환 \n",
    "    predict_sentence = tokenizer.decode(\n",
    "                            [i for i in prediction if i < tokenizer.vocab_size])\n",
    "    print('입력 질문 : ',sentence)\n",
    "    print('챗봇 대답 : ',predict_sentence)\n",
    "    return predict_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a9368f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자-숫자 사전(Voca) 불러오기\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3750695a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = pickle.load(open('token.sav','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac44f931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 과정이 끝나면 그녀에게 고백할꺼야!\n",
      "입력 질문 :  이 과정이 끝나면 그녀에게 고백할꺼야!\n",
      "챗봇 대답 :  직접 물어보세요 .\n",
      "직접 물어보세요 .\n"
     ]
    }
   ],
   "source": [
    "# 실제 질문 문장 입력 \n",
    "input_sentance = input()\n",
    "output = predict(input_sentance)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
